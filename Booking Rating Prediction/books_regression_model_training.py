# -*- coding: utf-8 -*-
"""Books regression model training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12t9Ypewdm8CASWtgyrFAGk_F36LpAeBE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from math import floor

import pickle
import warnings

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import LinearSVR, SVR

from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.metrics import r2_score,mean_squared_error

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
np.random.seed(24680)
warnings.filterwarnings("ignore")

"""# Training a prediction model for the average rating of a book
---
### Motivation
This project will serve as an item based recommendation system for the average rating of a book. Thus no new user preferences will be taken into account. We will solely rely on the already rated books.

### Data source
The dataset is a sample from Goodreads.com and is published at: https://www.kaggle.com/jealousleopard/goodreadsbooks
"""

books_data = pd.read_csv("/content/drive/MyDrive/College/8th Sem CSE Notes/8 Sem Notes/ML/Lab/datasets/books.csv",
                         error_bad_lines=False,
                         warn_bad_lines=False)

"""### Columns description: 
- **bookID** unique Identification number for each book.
- **title** The name under which the book was published.
- **authorsNames** of the authors of the book. Multiple authors are delimited with -.
- **average_rating** The average rating of the book received in total.
- **isbn** Another unique number to identify the book, the International Standard Book Number.
- **isbn13** A 13-digit ISBN to identify the book, instead of the standard 11-digit ISBN.
- **language_code** Helps understand what is the primary language of the book. For instance, eng is standard for English.
- **\# num_pages** Number of pages the book contains.
- **ratings_count** Total number of ratings the book received.
- **text_reviews_count** Total number of written text reviews the book received.
"""

books_data.sample(5)

books_data.shape

books_data.dtypes

books_data.isna().any().any()

"""There are no NA values in the dataset."""

books_data.rename(columns={"# num_pages":"pages_count"}, inplace=True)

books_data.sample(5)

"""Also bookID, isbn and isbn13 are just unique identifiers so I will drop them as they will not provide any additional information."""

books_data = books_data.drop(["bookID", "isbn", "isbn13"], axis = 1)
books_data.sample(5)

print("The following heatmap displays the correlation between the features:")

sns.heatmap(data=books_data.corr(),
            linewidths=0.25, square=True,
            linecolor="black", annot=True)

"""We see a high correlation between the ratings_count and the text_reviews_count (~ 86%). From this 
we can conclude that when a person writes a review he/she will most likely also rate the book itself.

Let's examine the top 10 most rated books.
"""

most_rated = books_data.sort_values(by="ratings_count", ascending = False).head(10)

most_rated_titles = pd.DataFrame(most_rated.title).join(pd.DataFrame(most_rated.ratings_count))
most_rated_titles

fig, ax = plt.subplots(figsize=(8, 8))
ax.invert_yaxis()

y_axis = most_rated_titles.title
x_axis = most_rated_titles.ratings_count

plt.barh(y_axis,x_axis, align="center")
plt.title('Top 10 Books: Most Rated')
plt.ylabel('Titles')
plt.xlabel('Ratings count(In Millions)')

def scatter_plot(x, y, title, x_label, y_label):
    plt.subplots(figsize=(8, 8))
    plt.scatter(x,
                y)
    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel(y_label)

pages_count_and_average_rating_title = "Relation between pages count and average rating"
average_rating_label = "Average rating"
pages_count_label = "Pages count"
scatter_plot(books_data.average_rating,
             books_data.pages_count,
            pages_count_and_average_rating_title, average_rating_label, pages_count_label)

"""There are a lot of outliers here. We can see that is no significant relation between average rating and the count of pages of a book.

The heatmap also proved this as it displayed a ration of 0.17.
Will drop the ourlier (with pages count >= 2000).
"""

books_data = books_data.drop(books_data.index[books_data["pages_count"] >= 2000])
pages_count_and_average_rating_title = "Relation between pages count and average rating"
average_rating_label = "Average rating"
pages_count_label = "Pages count"
scatter_plot(books_data.average_rating,
             books_data.pages_count,
            pages_count_and_average_rating_title, average_rating_label, pages_count_label)

print("Let`s focus on books ranging between 1-1500 pages.")
limited_by_page_count_books = books_data[books_data.pages_count <= 1500]
limited_by_page_count_books

scatter_plot(limited_by_page_count_books.average_rating,
             limited_by_page_count_books.pages_count,
             pages_count_and_average_rating_title,
             average_rating_label,
             pages_count_label)

plt.subplots(figsize=(8, 8))
plt.title("People preferences in regards to pages count ")
violionplot = plt.violinplot(limited_by_page_count_books.pages_count, showmeans=True, points=1000, widths=1)
plt.xticks([], None)

"""We can conclude that people tend to prefer books with pages count between 200 and 400."""

books_data[books_data["pages_count"] == 0]

"""There are 0 values in the pages_count column. Will replace them with the mean of the column."""

plt.subplots(figsize=(8,8))
plt.boxplot(books_data.pages_count)
plt.xticks([1], ["Pages count"])
plt.title("Boxplot of count of pages")

print("Mean of pages count", float(floor(books_data.pages_count.mean())))

books_data.pages_count = books_data.pages_count.replace(0, np.nan)
books_data.pages_count.fillna(float(floor(books_data.pages_count.mean())), inplace=True)

print("Count of 0s in pages count column:", len(books_data[books_data["pages_count"] == 0]))
print("Are there any NaNs in pages count column:" , books_data.pages_count.isna().any().any())

books_data

plt.subplots(figsize=(8,8))
plt.boxplot(books_data.ratings_count)
plt.xticks([1], ["Ratings count"])
plt.title("Boxplot of count of ratings")

"""There are a few  ourliers here. I will leave them as they are the most popular books (i.e Harry Potter and Twilight).

I`m going to encode all the string and categorical variables.
"""

encoder = LabelEncoder()
books_data.title = encoder.fit_transform(books_data.title)
books_data.sample(5)
books_data.authors = encoder.fit_transform(books_data.authors)

books_data = pd.get_dummies(books_data)

books_data.sample(5)

books_data.shape

plt.subplots(figsize=(8,8))
plt.boxplot(books_data.pages_count)
plt.xticks([1], ["Pages count"])
plt.title("Boxplot of count of pages")

"""Now we will focus no the target (i.e the average_rating). A histogram is a good way to visualize the sample."""

plt.subplots(figsize=(8,8))
plt.hist(books_data.average_rating,density=True)
plt.xlabel("Average rating")
plt.ylabel("Density")
plt.title("Histogram of the average rating")

"""From the histogram we see that most of ratings vary from 3 to 4.5. We can conclude that people tend to like the books they rate thus most of the ratings are very high.

### Chosing the best model for regression
I'm going to try whether we can use the non-modified for predictions. If the scores are not good I will try to transform the data using a polynom and repeat the same steps and parameter tuning.
"""

book_data=StandardScaler().fit_transform(books_data)
books_data_attributes =books_data.drop("average_rating", axis =1)
books_data_labels =books_data.average_rating

X_train, X_test, y_train, y_test = train_test_split (books_data_attributes, books_data_labels, test_size = 0.3)

k_fold = list(KFold(n_splits=5, shuffle=True).split(X_train, y_train))

def grid_search_best_model(model, params, k_fold, X_train, y_train):
    grid_search = GridSearchCV(model,
                           params,                             
                          cv=k_fold).fit(X_train,y_train)
    print("Best params", grid_search.best_params_)
    print("Best estimator", grid_search.best_estimator_)
    print("Best score:", grid_search.best_score_)
    
    return grid_search.best_estimator_

model_results = {}

def score_model(model,X_train, X_test, y_train, y_test,
               show_plot=True):   
    y_pred = model.predict(X_test)  
    print(f"Training score: {model.score(X_train,y_train)}")
    print(f"Test score: {r2_score(y_test, y_pred)}")
    print("MSE: ", mean_squared_error(y_test, y_pred))
    
    predictions_comparision = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': y_pred.tolist()}).sample(25)
    if show_plot == True:
        predictions_comparision.plot(kind="bar", figsize=(12,8),title="Actual vs predicted values")
    print(predictions_comparision.sample(10))    
    
    
    return {
        "training_score": model.score(X_train,y_train),
        "test_score_r2" : r2_score(y_test, y_pred),
        "test_score_mse" : mean_squared_error(y_test, y_pred)
    }

def compare_results():
    for key in model_results:
        print("Regression: ", key)
        print("Trainign score", model_results[key]["training_score"])
        print("R2 Test score ", model_results[key]["test_score_r2"])
        print("MSE Test score ", model_results[key]["test_score_mse"])
        print()

params={
    "fit_intercept":[True,False],
}

linear_regression = grid_search_best_model(LinearRegression(), params, k_fold, X_train, y_train)
model_results["linear_regression"] = score_model(linear_regression, X_train, X_test, y_train, y_test)

params={
    "n_neighbors": range(2, 30),
    "leaf_size":[20,30,50,70]
}

knn = grid_search_best_model(KNeighborsRegressor(), params, k_fold, X_train, y_train)
model_results["knn"] = score_model(knn, X_train, X_test, y_train, y_test)

params = {    "min_samples_split": [10, 20, 40],
              "max_depth": [2, 6, 8, 15, 50],
              "min_samples_leaf": [5, 20, 30],
              "max_leaf_nodes": [5, 20],
              }
dtr = grid_search_best_model(DecisionTreeRegressor(), params, k_fold, X_train, y_train)
model_results["dtr"] = score_model(dtr, X_train, X_test, y_train, y_test)

params = {"learning_rate":[0.3, 0.5,1],
          "n_estimators": [50, 100,200,400,700,1000]
              }
abr = grid_search_best_model(AdaBoostRegressor(), params, k_fold, X_train, y_train)
model_results["abr"] = score_model(abr, X_train, X_test, y_train, y_test)

"""The data is normalized using the standar scaler. Will try to use relatively small C."""

params = {
  "C": [0.1,1, 5, 10, 150,500,1000,5000],
    "fit_intercept":[True,False]
}
linear_svr = grid_search_best_model(LinearSVR(), params,k_fold, X_train, y_train)
model_results["linear_svr"] = score_model(linear_svr, X_train, X_test, y_train, y_test)

params={"n_estimators":[20, 50, 100,200], 
        "learning_rate": [0.01, 0.05, 0.1, 0.3],
         "max_depth":[3,5,10], 
        "min_samples_leaf": [3,5],
          "max_features": [0.3, 1]
       }        

gbr = grid_search_best_model(GradientBoostingRegressor(), params,k_fold, X_train, y_train)
model_results["gbr"] = score_model(gbr, X_train, X_test, y_train, y_test)

params = { 
    "C": [1, 5, 50, 100],
    "gamma": [0.001, 0.01, 0.1],
    "epsilon" : [0.01, 0.1]
}

gaussian_svr = grid_search_best_model(SVR(), params, k_fold,X_train, y_train)
model_results["gaussian_svr"] = score_model(gaussian_svr, X_train, X_test, y_train, y_test)

"""From what we can see the score of all algorithms are not very good. Plus some overfit. I will try
to transform the data with polynom of 2nd degree in order to improve the scores.
"""

quad_transformer= PolynomialFeatures(degree=2, interaction_only=True)
books_data_attributes_quad_transformed = quad_transformer.fit_transform(books_data_attributes)
X_train_quad_transformed, X_test_quad_transformed, y_train_quad_transformed, y_test_quad_transformed = train_test_split (books_data_attributes_quad_transformed, books_data_labels, test_size = 0.3)
k_fold_quad_transformed = list(KFold(n_splits=5, shuffle=True).split(X_train_quad_transformed, y_train_quad_transformed))

params={
    "fit_intercept":[True,False],
}

linear_regression_quad_transformed = grid_search_best_model(LinearRegression(), params, k_fold_quad_transformed,X_train_quad_transformed, y_train_quad_transformed)
model_results["linear_regression_quad_transformed"] = score_model(linear_regression_quad_transformed, X_train_quad_transformed, X_test_quad_transformed, y_train_quad_transformed, y_test_quad_transformed)

params = {"learning_rate":[0.3, 0.5,1],
          "n_estimators": [50, 100,200,400]}
              
abr_quad_transformed = grid_search_best_model(AdaBoostRegressor(), params,k_fold_quad_transformed,  X_train_quad_transformed, y_train_quad_transformed)
model_results["abr_quad_transformed"] = score_model(abr_quad_transformed, X_train_quad_transformed, X_test_quad_transformed, y_train_quad_transformed, y_test_quad_transformed)

params={"n_estimators":[20, 50, 100,200,400], 
        "learning_rate": [0.01, 0.05, 0.1, 0.3],
         "max_depth":[3,5,10], 
        "min_samples_leaf": [3,5],
        "max_features": [0.3, 1]
       }        

gbr_quad_transformed = grid_search_best_model(GradientBoostingRegressor(), params,k_fold_quad_transformed,  X_train_quad_transformed, y_train_quad_transformed)
model_results["gbr_quad_transformed"] = score_model(gbr_quad_transformed,  X_train_quad_transformed,
                                                    X_test_quad_transformed, y_train_quad_transformed, y_test_quad_transformed)

"""### Comparing the results"""

compare_results()

"""From the observations we see that a GradientBoostingRegressor with preprocessed data (using polynoms of 2nd degree) performed the best on the test data with score like ~19%. On the training data it did good - around 53%. However the overall scores are not very good but the actual vs. predicted data printed displays some satisfying results.

### Serializing the best model
"""

model_dump_filename = "books_model.pkl"
pickle.dump(gbr_quad_transformed, open(model_dump_filename, "wb"))
print("Successfully dumped the model to", model_dump_filename)